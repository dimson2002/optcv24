# optcv24
# Задание 2.

Подготовка окружения и обучение простого классификатора на учебном датасете, например, cifar10.

Асеев Дмитрий, SGD, Adagrad.

Главными плюсами AdaGrad является адаптивная скорость обучения и преимущество на разреженных данных (если есть признаки, которые появляются нечасто, то оптимизатор будет давать этим признакам более высокий шаг, тем самым более значительно обновляя их).
А главным минусом является невозможность "перезагрузить" шаг в отличие от RMSProp или Adam. Adagrad не включает механизм, который бы "обнулял" или сглаживал накопленное значение градиента, из-за чего модель может существенно замедлится.

Я вдохновился идеей Валерия Рябинкина по запуску функции train два раза, поэтому я создал условие выбора оптимизатора:

    if optimizer_name == 'SGD':
        optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
    elif optimizer_name == 'Adam':
        optimizer = torch.optim.Adam(net.parameters(), lr=0.001)

Также в функцию добавляется логгирование потерь и считается средняя потеря за эпоху.

И дальше отрисовывается график.

И главное запустить функцию с два раза подряд:

    if __name__ == '__main__':
        sgd_losses = train(optimizer_n='SGD')
        adagrad_losses = train(optimizer_n='Adagrad')

        losses_dict = {'SGD': sgd_losses,
                   'Adagrad': adagrad_losses}
        plot_loss(losses_dict)

![loss_plot_comparison.png](loss_plot_comparison.png)


*ВЫВОД:* при данных условиях оптимизатор SGD показывает лучшую динамику обучения, более быстрое и более глубокое снижение функции потерь по сравнению с Adagrad.

**ПОЧЕМУ ТАК ВЫШЛО?**

Adagrad сильно адаптирует шаг обучения, и в данноом случае он, вероятно, слишком быстро становится маленьким. Это приводит к медленной сходимости по сравнению с более "агрессивным" и постоянным шагом у SGD.